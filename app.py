# app.py: RegNarrative Auditor Streamlit Frontend (Upgraded)
# Generated by RegNarrative_Auditor_Factory.ipynb

import streamlit as st
import os
import time
import logging
from typing import List, Optional
import tempfile

from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores.pgvector import PGVector
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from tenacity import retry, stop_after_attempt, wait_exponential

# New Document Loaders
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, UnstructuredURLLoader, UnstructuredFileLoader

# --- Configuration ---
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY")
DB_CONNECTION_STRING = st.secrets.get("DB_CONNECTION_STRING")
COLLECTION_NAME = "reg_narrative_audits"

if not GEMINI_API_KEY or not DB_CONNECTION_STRING:
    st.error("Missing secrets: GEMINI_API_KEY or DB_CONNECTION_STRING. Please set them in your Streamlit app settings.")
    st.stop()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Database & RAG Setup (Cached) ---

@st.cache_resource
def get_embeddings():
    logging.info("Loading Google Generative AI Embeddings model...")
    return GoogleGenerativeAIEmbeddings(
        model="text-embedding-004", 
        google_api_key=GEMINI_API_KEY
    )

@st.cache_resource
def get_vectorstore(_embeddings):
    logging.info("Initializing vector store connection with retry...")
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def connect_to_pgvector():
        # This is a lazy constructor. We just create the object.
        # The *actual* connection will be tested on the first query.
        vectorstore = PGVector(
            connection_string=DB_CONNECTION_STRING,
            embedding_function=_embeddings,
            collection_name=COLLECTION_NAME
        )
        # We removed the faulty 'get_relevant_documents("test")' line.
        return vectorstore
        
    try:
        return connect_to_pgvector()
    except Exception as e:
        st.error(f"System Failed to Initialize. Check DB connection and secrets: {e}")
        logging.error(f"Failed to connect to PGVector: {e}")
        st.stop()

@st.cache_resource
def get_rag_chain(_vectorstore):
    logging.info("Setting up RAG chain...")
    retriever = _vectorstore.as_retriever(search_kwargs={"k": 5})
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", 
        temperature=0.1, 
        google_api_key=GEMINI_API_KEY
    )

    # --- NEW FORMATTED SYSTEM INSTRUCTION ---
    SYSTEM_PROMPT = """
You are the RegNarrative Auditor Agent. Your task is to perform a multi-hop compliance audit based ONLY on the retrieved context.

You MUST format your response in clean, readable Markdown using the following structure.
Ensure all text is properly spaced and uses newlines for readability.

**Audit Conclusion:** [State if the action is Compliant or Non-Compliant]

**Reasoning:** [Provide a detailed explanation for your conclusion, referencing the context. Use full, well-spaced sentences.]

**Consequence of Non-Compliance:** [If non-compliant, state the specific consequence from the context (e.g., "An immediate internal audit and a penalty of 5% of the transaction value."). If compliant, state "N/A".]

**Citations:**
- [Full name of the policy or regulation (e.g., Regulation 401.A)] (Source: [source_file_name.txt])
- [Full name of any other policy] (Source: [other_file_name.txt])

---
RULES:
1.  Base your answer *only* on the text provided in the "Retrieved Context".
2.  Do not make up information. If the context is insufficient, state that.
3.  Ensure your reasoning directly quotes or paraphrases the provided context with correct spacing.
"""

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SYSTEM_PROMPT),
            ("human", "Audit Query: {query}\n\nRetrieved Context:\n{context}"),
        ]
    )

    def format_docs(docs):
        return "\n\n---\n\n".join([f"Source ({doc.metadata.get('source', 'Unknown')}): {doc.page_content}" for doc in docs])

    rag_chain = (
        {"context": retriever | format_docs, "query": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# --- Document Processing Functions ---

def load_and_split_documents(uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile], urls: List[str]) -> List[Document]:
    docs_to_split = []
    
    # Process file uploads
    for file in uploaded_files:
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf" if "pdf" in file.type else ".txt") as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_file_path = tmp_file.name
            
            if "pdf" in file.type:
                loader = PyPDFLoader(tmp_file_path)
            else:
                # Use UnstructuredFileLoader for .txt, .md, etc.
                loader = UnstructuredFileLoader(tmp_file_path)
                
            loaded_docs = loader.load()
            # Manually add the filename to the metadata
            for doc in loaded_docs:
                doc.metadata["source"] = file.name
            docs_to_split.extend(loaded_docs)
            
            os.remove(tmp_file_path) # Clean up temp file
        except Exception as e:
            st.warning(f"Could not load file {file.name}: {e}")
            
    # Process URLs
    if urls:
        try:
            url_loader = UnstructuredURLLoader(urls=urls, mode="single")
            docs_to_split.extend(url_loader.load())
        except Exception as e:
            st.warning(f"Could not load URL(s): {e}")
            
    # Split the documents
    if docs_to_split:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        splits = text_splitter.split_documents(docs_to_split)
        return splits
    return []

@st.cache_data(ttl=600) # Cache the source list for 10 minutes
def get_knowledge_base_sources(_vectorstore: PGVector) -> List[str]:
    # This is a bit of a hack, since PGVector doesn't have a simple "list all metadata" function.
    # We'll retrieve a large number of documents and dedupe their sources.
    try:
        results = _vectorstore.similarity_search(" ", k=50) # Retrieve 50 docs
        sources = set(doc.metadata.get("source", "Unknown").split('/')[-1] for doc in results)
        return sorted(list(sources))
    except Exception as e:
        logging.error(f"Could not query sources: {e}")
        return ["Error querying database."]

# --- Load Models & Get RAG Chain ---
embeddings = get_embeddings()
vectorstore = get_vectorstore(embeddings)
rag_chain = get_rag_chain(vectorstore)

# --- Streamlit UI ---

st.set_page_config(layout="wide", page_title="RegNarrative Auditor")

# --- Sidebar for Document Management ---
with st.sidebar:
    st.header("Manage Knowledge Base")
    
    with st.form("add_docs_form", clear_on_submit=True):
        uploaded_files = st.file_uploader(
            "Upload new documents", 
            type=["pdf", "txt", "md"], 
            accept_multiple_files=True
        )
        url_input = st.text_input("...or add a public URL")
        
        submitted = st.form_submit_button("Add to Knowledge Base")
        
        if submitted:
            urls_to_load = [url_input] if url_input else []
            if not uploaded_files and not urls_to_load:
                st.warning("Please upload a file or enter a URL.")
            else:
                with st.spinner("Processing and embedding new documents..."):
                    new_splits = load_and_split_documents(uploaded_files, urls_to_load)
                    if new_splits:
                        vectorstore.add_documents(new_splits)
                        st.success(f"Added {len(new_splits)} new knowledge chunks to the database!")
                        # Clear the source cache
                        get_knowledge_base_sources.clear()
                        st.rerun() # Rerun to update the sidebar
                    else:
                        st.error("Failed to load or process any documents.")
    
    st.divider()
    st.subheader("Current Sources in Database")
    
    sources = get_knowledge_base_sources(vectorstore)
    if sources:
        for source in sources:
            st.markdown(f"- `{source}`")
    else:
        st.markdown("No documents found in knowledge base.")

# --- Main Chat Interface ---
st.title("üõ°Ô∏è RegNarrative Auditor: Agentic Compliance System")
st.caption("Verifiable, production-grade RAG powered by Gemini and pgvector. Add new documents via the sidebar.")

if 'messages' not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if audit_query := st.chat_input("Enter Audit Query or Transaction Scenario..."):
    st.session_state.messages.append({"role": "user", "content": audit_query})
    with st.chat_message("user"):
        st.markdown(audit_query)

    with st.chat_message("assistant"):
        with st.spinner("Running agentic, multi-hop RAG..."):
            try:
                result = rag_chain.invoke(audit_query)
                st.markdown(result)
                st.session_state.messages.append({"role": "assistant", "content": result})
                
            except Exception as e:
                st.error(f"An error occurred during audit execution: {e}")
                logging.error(f"RAG Chain Execution Error: {e}")